{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports"
      ],
      "metadata": {
        "id": "koqUWT4cv7NV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNwlZgJrXurb"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FxluC3ylbxkp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQASGFNVWJE9"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Ph3lYVFow-6H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "yjPq2dI75fb7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запросы имеют следующий вид в зависимости от параметра shots:"
      ],
      "metadata": {
        "id": "iP39B9ws4_yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing model and dataset"
      ],
      "metadata": {
        "id": "gsQGbha9wPw_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFAIf7vXT8NJ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"mbpp\")\n",
        "dataset = dataset['test']\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "# #model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.float16, device_map='auto', load_in_8bit=True)"
      ],
      "metadata": {
        "id": "5F2zzrtY0Hdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto', load_in_8bit=True)"
      ],
      "metadata": {
        "id": "3hn0pB8jY3At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation and data preparation functions"
      ],
      "metadata": {
        "id": "-tyALauUwbrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt_list(dataset, shots, num_tests):\n",
        "    prompt_list = []\n",
        "    pattern = r'(?<=assert\\s)\\w+\\s*\\('\n",
        "    for i in range(num_tests):\n",
        "        s = dataset['test_list'][i][0]\n",
        "        func_name = re.search(pattern, s)\n",
        "        func_name = func_name.group().strip(' (') if func_name else ''\n",
        "        prompt = dataset['text'][i] + ' The function should have the following name: ' + func_name + '.\\n'\n",
        "        if shots > 0:\n",
        "            prompt += 'The code should also pass these tests: '\n",
        "            for j in range(shots):\n",
        "                prompt +=  dataset['test_list'][i][j] + ', '\n",
        "            prompt = prompt[:len(prompt) - 2]\n",
        "        prompt_list.append(prompt)\n",
        "    return prompt_list\n",
        "\n",
        "def generate(model, dataset, shots=0, num_tests=len(dataset), do_sample=False, top_p=-1.0, top_k=0, temperature=1.0):\n",
        "    prompt_list = make_prompt_list(dataset, shots, num_tests)\n",
        "    model.eval()\n",
        "    responses = []\n",
        "\n",
        "    for prompt in prompt_list:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        print(prompt)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=200, num_return_sequences=1, do_sample=do_sample, top_p=top_p, top_k=top_k, temperature=temperature, eos_token_id=tokenizer.eos_token_id)\n",
        "            #outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=128, do_sample=True, top_p=0.9, temperature=0.1)\n",
        "            response = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "\n",
        "        responses.append(response)\n",
        "\n",
        "    return prompt_list, responses"
      ],
      "metadata": {
        "id": "Y6E_Vz4gwfuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_list = make_prompt_list(dataset, 0, 5)\n",
        "print(prompt_list[0])\n",
        "prompt_list = make_prompt_list(dataset, 3, 5)\n",
        "print(prompt_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZR9Pkp00s1E",
        "outputId": "1dc08ab0-9104-4568-e656-4549a5213a86"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Write a python function to remove first and last occurrence of a given character from the string. The function should have the following name: remove_Occ.\n",
            "\n",
            "3\n",
            "Write a python function to remove first and last occurrence of a given character from the string. The function should have the following name: remove_Occ.\n",
            "The code should also pass these tests: assert remove_Occ(\"hello\",\"l\") == \"heo\", assert remove_Occ(\"abcda\",\"a\") == \"bcd\", assert remove_Occ(\"PHP\",\"P\") == \"H\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "d_R4IYlI_TWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc0veys6iRC9"
      },
      "outputs": [],
      "source": [
        "def extract_function(text):\n",
        "  function_lines = []\n",
        "  inside_function = False\n",
        "\n",
        "  for line in text.split('\\n'):\n",
        "    if line.startswith('import'):\n",
        "      function_lines.append(line)\n",
        "      inside_function = True\n",
        "    elif line.startswith('def') and inside_function == False:\n",
        "      function_lines.append(line)\n",
        "      inside_function = True\n",
        "    elif inside_function:\n",
        "      if line == '```' or line == \"\" or line == '\\end{code}':\n",
        "        function_code = '\\n'.join(function_lines)\n",
        "        return function_code\n",
        "      else:\n",
        "        function_lines.append(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YL6BhGv1iT0t"
      },
      "outputs": [],
      "source": [
        "def test(num_tests, dataset, responses):\n",
        "  score = 0\n",
        "  codes = []\n",
        "  tests = []\n",
        "  results = []\n",
        "  for i in range(num_tests):\n",
        "    code = extract_function(responses[i])\n",
        "    codes.append(code)\n",
        "    test = dataset['test_list'][i][0] + '\\n' + dataset['test_list'][i][1] + '\\n' + dataset['test_list'][i][2]\n",
        "    tests.append(test)\n",
        "    code = code + '\\n' + test if code else test\n",
        "    code = code.strip()\n",
        "    flag = True\n",
        "    try:\n",
        "      exec(code)\n",
        "    except (AssertionError, TypeError, IndentationError, NameError, SyntaxError):\n",
        "      flag = False\n",
        "      pass\n",
        "    else:\n",
        "      score += 1\n",
        "    results.append('Ok' if flag else 'Error')\n",
        "  return codes, tests, results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(df, parameters, prompt_list, responses, codes, tests, results):\n",
        "    bias = len(df)\n",
        "    df.loc[bias, 'parameters'] = parameters\n",
        "    for i in range(len(prompt_list)):\n",
        "        df.loc[bias + i, 'prompt'] = prompt_list[i]\n",
        "        df.loc[bias + i, 'response'] = responses[i]\n",
        "        df.loc[bias + i, 'code'] = codes[i]\n",
        "        df.loc[bias + i, 'tests'] = tests[i]\n",
        "        df.loc[bias + i, 'result'] = results[i]"
      ],
      "metadata": {
        "id": "D2auaFmWUXNj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(df, model, dataset, shots=0, num_tests=100, do_sample=False, top_p=-1.0, top_k=0, temperature=1.0):\n",
        "    prompt_list, responses = generate(model=model, dataset=dataset, shots=shots, num_tests=NUM_TESTS, do_sample=False, top_p=top_p, top_k=top_k, temperature=temperature)\n",
        "    codes, tests, results = test(dataset=dataset, num_tests=NUM_TESTS, responses=responses)\n",
        "    parameters = str({'shots': shots, 'do_sample': do_sample, 'top_p': top_p, 'top_k': top_k, 'temperature': temperature})\n",
        "    save_results(df, parameters, prompt_list, responses, codes, tests, results)"
      ],
      "metadata": {
        "id": "Me_N14H6N-c5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting results"
      ],
      "metadata": {
        "id": "3PvOWkOq0cCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TESTS = 10"
      ],
      "metadata": {
        "id": "vPh3YBvn20U7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns = ['parameters', 'prompt', 'response', 'code', 'tests', 'result'])\n",
        "\n",
        "for shots in [0, 1, 2]:\n",
        "    predict(df=df, model=model, dataset=dataset, shots=shots, num_tests=NUM_TESTS, do_sample=False)\n",
        "\n",
        "for shots in [0, 1, 2]:\n",
        "    for top_p in [0.9, 0.92, 0.95]:\n",
        "        for top_k in [30, 40, 50]:\n",
        "            for temperature in [0.01, 0.05, 0.1, 0.25]:\n",
        "                predict(df, model=model, dataset=dataset, shots=shots, num_tests=NUM_TESTS, do_sample=True, top_p=top_p, temperature=temperature)"
      ],
      "metadata": {
        "id": "AtVu0WeWQQLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving results"
      ],
      "metadata": {
        "id": "0WXjVcrJElVq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MwWY22PZcUTg"
      },
      "outputs": [],
      "source": [
        "df.to_csv('data.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}